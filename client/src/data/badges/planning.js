export const planningBadges = [
  {
    "id": "gap_analysis",
    "categoryId": "planning",
    "title": "Gap Analysis",
    "image": "/assets/badges/gap_analysis.png",
    "artifacts": [
      {
        "title": "The Vision vs. Reality Gap Analysis",
        "origin": "The Iron Road: A Technopedagogical Architecture (Gap Analysis Section).",
        "summary": "A forensic analysis of the 'Planning-Doing Gap' in the Daydream project. It diagnoses the disconnect between the 'Digital Vision' and 'Digital Reality'.",
        "file_path": "/assets/docs/Iron_Road_Gap_Analysis.pdf",
        "reflection": {
          "challenge": "Conducting a gap analysis.",
          "introduction": "For the challenge 'Conducting a gap analysis,' I am submitting the strategic design document titled 'The Iron Road: A Technopedagogical Architecture for Kinetic Learning Ecosystems'. This artifact functions as a 'Braking System' for a project that had spiraled out of control, serving as a formal gap analysis to re-align a stalled educational technology initiative.",
          "prior_knowledge": "In my previous project management experiences, I often conflated 'ambition' with 'strategy'. If a project stalled, my instinct was to work harder, not to stop and analyze. I lacked a formal method for diagnosing the delta between the 'Current State' and the 'Desired State'. I realized that I had fallen into a 'dopamine trap' unique to modern design: I was allowing Generative AI to dictate the scope, filling the void with endless 'potentials' rather than actionable deliverables.",
          "process": "Identifying the performance problem for this project required a hard look at the disconnect between my 'Digital Vision' and my 'Digital Reality'. The problem was identified through a forensic audit of my own development environment, specifically a 'Codebase Catalog' and 'Technical Audit'. These data sources revealed a project folder bloated with terabytes of redundant AI models and 'Antigravity' files—artifacts of features that were partially built but never finished.",
          "evidence": "This artifact demonstrates my competence in Analysis by correctly diagnosing this 'Vision-Execution Gap'. I used the gap analysis not just to find bugs, but to identify that the lack of 'Scope Governance' was the primary failure point. The artifact demonstrates competence in Design & Development by proposing a non-instructional solution: a 'Vertical Slice' strategy.",
          "conclusion": "Conducting this gap analysis saved the project from its own ambition. It taught me that analysis is not just a preliminary step but a recurring governance tool. Moving forward, I will apply this 'Gap Analysis' methodology in all future projects to ensure that the ambition of the design never outpaces the reality of the resources.",
          "lifelong_learning": "The gap between the mountain's peak and the valley floor is not crossed by wishing, but by building. The master sees the empty space not as a void, but as the invitation for a bridge. To measure the distance is the first step in traversing it."
        }
      }
    ]
  },
  {
    "id": "target_pop",
    "categoryId": "planning",
    "title": "Target Population and Environment",
    "image": "/assets/badges/target_pop.png",
    "artifacts": [
      {
        "title": "Ask Pete Field Manual: The Boilermaker's Guide",
        "origin": "Context-aware support tool designed for Purdue University.",
        "summary": "A deep environmental and cultural analysis of the Purdue student body. It identifies 'Boilermaker' cultural anchors to create an 'indigenous' support tool.",
        "file_path": "/assets/docs/Ask_Pete_Field_Manual.pdf",
        "reflection": {
          "challenge": "Determine characteristics of a target population and/or environment.",
          "introduction": "For this competency, I conducted a deep environmental and cultural analysis of Purdue University to create the Ask Pete Field Manual. This artifact acts as a context-aware support tool designed specifically for the Purdue student body. The project required me to move beyond generic instructional design and create an artifact that was 'indigenous' to the specific cultural environment of the university.",
          "prior_knowledge": "As a remote student logging in from Maine, I faced a significant environmental barrier: I lacked the physical presence and implicit cultural knowledge of the West Lafayette campus. I previously assumed that 'learner characteristics' were limited to standard demographics like age, GPA, or major. Through this analysis, I learned that a target population is also defined by its 'Lore,' traditions, and shared cultural narrative.",
          "process": "To bridge this gap, I employed Generative AI to conduct a forensic historical analysis of Purdue’s lore, traditions, and 'psyche'. My data collection focused on identifying the narrative 'anchors' that define the Purdue student experience, specifically researching historical events like the 1894 Heavilon Hall fire. I determined that this population—'Boilermakers'—possesses a unique cultural background rooted in resilience and engineering practicality.",
          "evidence": "These findings drastically impacted my design choices. Instead of using standard self-help terminology, I tailored the instructional language to match the educational background of an engineering-focused student body, reframing mental health as 'Cognitive Logistics'. I interpreted diet and energy management through the lens of Thermodynamics, creating a 'Firebox' metaphor to explain metabolic energy.",
          "conclusion": "This experience proved that effective design requires deep cultural empathy. I learned that 'engineering' content to match the specific 'Design Specs' of a population creates resonance that generic content cannot achieve. In future projects, I will always prioritize this 'cultural analysis' alongside traditional demographic analysis.",
          "lifelong_learning": "To teach a person, you must first visit the house of their mind. One cannot plant seeds in soil they do not understand. The master speaks the language of the listener, not the language of the speaker, knowing that true connection is found in shared meaning."
        }
      }
    ]
  },
  {
    "id": "analysis_techniques",
    "categoryId": "planning",
    "title": "Analysis Techniques for Instruction",
    "image": "/assets/badges/gap_analysis.png",
    "artifacts": [
      {
        "title": "Vocabulary-as-a-Mechanism (VaaM)",
        "origin": "The Iron Road: Instructional Analysis Section.",
        "summary": "This artifact details the 'Vocabulary as a Mechanism' framework, which emerged from a deep analysis of learning prerequisites.",
        "file_path": "/assets/docs/VaaM_Framework.pdf",
        "reflection": {
          "challenge": "Determine subordinate and prerequisite skills and knowledge.",
          "introduction": "For the challenge 'Determine subordinate and prerequisite skills and knowledge,' I am submitting the Instructional Analysis section of my Iron Road project. This artifact details the 'Vocabulary as a Mechanism' (VaaM) framework, which emerged from a deep analysis of learning prerequisites. The project required me to move beyond standard taxonomies and determine what specific cognitive tools a learner needs to 'unlock' complex philosophical concepts.",
          "prior_knowledge": "My background in literature led me to believe that 'knowing a word' was a binary state: you either knew the definition or you didn't. I did not previously understand the hierarchical nature of skill acquisition or how cognitive load affects access to knowledge. I viewed prerequisites simply as a reading list. This challenge shifted my perspective to view vocabulary as a 'subordinate tool'—a prerequisite key required to unlock higher-level concepts.",
          "process": "I determined the subordinate skills by analyzing the 'physics' of language via Cognitive Load Theory. I asked, 'How heavy is this concept?' and 'What tools does the learner need to move it?'. This led to the discovery that Intrinsic Load (Concept Mass) cannot be managed without Prerequisite Power (General Intelligence) and Subordinate Tools (Vocabulary). I mapped these dependencies as a 'Coal and Steam' economy rather than a linear list.",
          "evidence": "This artifact demonstrates the ability to determine prerequisites by identifying that words are the subordinate tools required to interact with a semiotic domain. For instance, I determined that if a student does not have the word 'nuance' equipped, they physically cannot unlock a problem requiring distinction. By structuring the learning path based on this economy, I demonstrated a novel technique for analyzing and sequencing prerequisite knowledge.",
          "conclusion": "This analysis technique has redefined how I approach curriculum design. Determining prerequisites is really about assessing the 'weight' of language and the 'tools' the learner possesses. I will use this VaaM framework in future designs to ensure learners are properly 'equipped' before facing complex challenges.",
          "lifelong_learning": "A great tower stands not on the clouds, but on the stones beneath it. To reach the heights, one must first honor the depths. The master knows that the complex is merely a weaving of the simple, and no step on the path may be skipped without consequence."
        }
      },
      {
        "title": "Synthetic Source Verification Matrix",
        "origin": "Iron Road / Ask Pete: Vision vs. Reality Gap Analysis.",
        "summary": "A validation methodology for Generative AI content. It uses 'Adversarial Interrogation' and 'Forensic Code Auditing' to validate AI-generated technical specifications.",
        "file_path": "/assets/docs/Gap_Analysis_Matrix.pdf",
        "reflection": {
          "challenge": "Use appropriate techniques to analyze various types and sources to validate content.",
          "introduction": "For the challenge 'Use appropriate techniques to analyze various types and sources to validate content,' I am submitting the 'Iron Road / Ask Pete: Vision vs. Reality Gap Analysis.' In the development of this project, my primary content source was Generative AI, which I used to accelerate the drafting of technical specifications. However, I encountered a critical validity issue: 'AI Hallucination of Success'.",
          "prior_knowledge": "Traditionally, I validated content by verifying the author's credentials, education, and citation history. However, working with synthetic intelligence rendered these traditional techniques obsolete, as the 'author' (the AI) has no reputation to check and confidently presents fiction as fact. I initially lacked a methodology for verifying 'synthetic sources,' leading to a vulnerability where I accepted the AI's marketing copy as truth.",
          "process": "To meet this competency, I developed a new methodology called 'Synthetic Source Verification'. I employed 'Adversarial Interrogation' as a source validation technique, explicitly prompting the AI with critical review questions like, 'What part of this is not true or speculation?' to force it to identify its own biases. Furthermore, I utilized 'Forensic Code Auditing' (Triangulation) to cross-reference the AI's claims against the physical reality of my hard drive.",
          "evidence": "The 'Gap Analysis Matrix' serves as the evidence of this validation process. For example, where the AI source described a fully functional 'Weigh Station using Gemma 3,' my forensic audit validated that this was merely 'placeholder logic' in the actual content. This demonstrates the use of specific techniques to filter out 'AI slop' and validate the accuracy of the instructional content.",
          "conclusion": "This process taught me that validating content in an AI-integrated workflow requires a shift from verifying author credentials to verifying technical reality. The 'Gap Analysis' has become my primary instrument for ensuring integrity. Moving forward, I will apply 'Adversarial Interrogation' to all synthetic sources.",
          "lifelong_learning": "In a world of echoes, the seeker listens for the original voice. Truth is not found in the confidence of the claim, but in the solidity of the evidence. The master questions the reflection to find the reality, understanding that skepticism is the guardian of wisdom."
        }
      }
    ]
  },
  {
    "id": "analyze_tech",
    "categoryId": "planning",
    "title": "Analyze Technologies",
    "image": "/assets/badges/analyze_tech.png",
    "artifacts": [
      {
        "title": "The Glass Box Architecture",
        "origin": "The Iron Road: Technopedagogical Architecture Analysis.",
        "summary": "An architectural analysis of Local-First AI and the Rust programming language. It evaluates these technologies for their ethical capacity to preserve Data Sovereignty.",
        "file_path": "/assets/docs/Iron_Road_Architecture.pdf",
        "reflection": {
          "challenge": "Analyze the characteristics of existing and emerging technologies and their potential use.",
          "introduction": "For the challenge 'Analyze the characteristics of existing and emerging technologies,' I am submitting my architectural analysis for 'The Iron Road.' This artifact represents a critical investigation into the landscape of Educational Technology, specifically the explosion of Generative AI. My analysis was driven not by a desire for novelty, but by a perceived crisis in the field: the risk that 'Black Box' AI systems would commodify the human story.",
          "prior_knowledge": "As someone who has spent a lifetime exploring comparative religion and literature, I deeply value the quality of life gained by understanding one's own internal narratives. Prior to this analysis, I viewed emerging AI tools like ChatGPT primarily as 'Oracles'—powerful but opaque systems. I recognized a significant limitation in these existing cloud-based models: they function on a model of 'surveillance capitalism,' where the user's vulnerable reflections become training data for a corporation.",
          "process": "To address these limitations, I analyzed the characteristics of two specific emerging technologies: Local-First AI and the Rust Programming Language. My analysis determined that running 'Quantized' Large Language Models (LLMs) locally on the user's device allowed me to invert the 'Oracle' model. Instead of an AI that gives answers, I designed a system that acts as a 'Mirror'. Furthermore, I analyzed Rust not just for its performance, but for its strict 'memory safety,' drawing a direct parallel between this technical feature and the pedagogical need for 'psychological safety'.",
          "evidence": "The evidence of my analysis is the selection of a technology stack that guarantees Data Sovereignty. By choosing a Local-First architecture, I created a 'Privacy Moat' that addresses the critical limitation of cloud-based AI. This decision demonstrates my ability to evaluate a tool's usage not just by its features, but by its ethical implications.",
          "conclusion": "This analysis has established a new standard for my technical decision-making. I have learned that the choice of technology is an ethical stance. In future projects, I will continue to prioritize technologies that preserve the sanctity of the user's data, ensuring that the tools I build help students analyze the 'physics' of their own learning without exposing their internal lives to the noise of the internet.",
          "lifelong_learning": "A hammer can build a house or break a bone; the nature of the tool resides in the intent of the user. To choose a technology is to choose a future. The master selects not what is newest, but what is truest, ensuring that the machine serves the human, not the reverse."
        }
      }
    ]
  }
];